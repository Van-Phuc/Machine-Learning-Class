{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\WCL-2017\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\WCL-2017\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\WCL-2017\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\WCL-2017\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "# import pandas as pd\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# libraries for dataset preparation, feature engineering\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "from sklearn import model_selection, preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "np.random.seed(123) #for reprodicible results\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "x_train = newsgroups_train.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_test = fetch_20newsgroups(subset='test')\n",
    "x_test = newsgroups_test.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = newsgroups_train.target\n",
    "y_test = newsgroups_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    text2 = \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in text]).split())\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text2) for word in nltk.word_tokenize(sent)]\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    stopwds = stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stopwds]\n",
    "    tokens = [word for word in tokens if len(word)>=3]\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    tagged_corpus = pos_tag(tokens)\n",
    "    Noun_tags = ['NN','NNP','NNPS','NNS']\n",
    "    Verb_tags = ['VB','VBD','VBG','VBN','VBP','VBZ']\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    def prat_lemmatize(token,tag):\n",
    "        if tag in Noun_tags:\n",
    "            return lemmatizer.lemmatize(token,'n')\n",
    "        elif tag in Verb_tags:\n",
    "            return lemmatizer.lemmatize(token,'v')\n",
    "        else:\n",
    "            return lemmatizer.lemmatize(token,'n')\n",
    "    pre_proc_text = \" \".join([prat_lemmatize(token,tag) for token,tag in tagged_corpus])\n",
    "    return pre_proc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_preprocessed = []\n",
    "for i in x_train:\n",
    "    x_train_preprocessed.append(preprocessing(i))\n",
    "x_test_preprocessed = []\n",
    "for i in x_test:\n",
    "    x_test_preprocessed.append(preprocessing(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=2, ngram_range=(1, 2), stop_words='english', max_features= 10000,strip_accents='unicode', norm='l2')\n",
    "x_train_2 = vectorizer.fit_transform(x_train_preprocessed).todense()\n",
    "x_test_2 = vectorizer.transform(x_test_preprocessed).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning modules\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import Adadelta,Adam,RMSprop\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition hyper parameters\n",
    "np.random.seed(1337)\n",
    "nb_classes = 20\n",
    "batch_size = 64\n",
    "nb_epochs = 20\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 1000)              10001000  \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 50)                50050     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 20)                1020      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 20)                0         \n",
      "=================================================================\n",
      "Total params: 10,052,070\n",
      "Trainable params: 10,052,070\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Deep Layer Model building in Keras\n",
    "model = Sequential()\n",
    "model.add(Dense(1000,input_shape= (10000,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "# model.add(Dense(500))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(50))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "11314/11314 [==============================] - 37s 3ms/step - loss: 1.8887\n",
      "Epoch 2/20\n",
      "11314/11314 [==============================] - 35s 3ms/step - loss: 0.6054\n",
      "Epoch 3/20\n",
      "11314/11314 [==============================] - 35s 3ms/step - loss: 0.3266\n",
      "Epoch 4/20\n",
      "11314/11314 [==============================] - 36s 3ms/step - loss: 0.2143\n",
      "Epoch 5/20\n",
      "11314/11314 [==============================] - 38s 3ms/step - loss: 0.1550\n",
      "Epoch 6/20\n",
      "11314/11314 [==============================] - 37s 3ms/step - loss: 0.1148\n",
      "Epoch 7/20\n",
      "11314/11314 [==============================] - 37s 3ms/step - loss: 0.0929\n",
      "Epoch 8/20\n",
      "11314/11314 [==============================] - 37s 3ms/step - loss: 0.0763\n",
      "Epoch 9/20\n",
      "11314/11314 [==============================] - 37s 3ms/step - loss: 0.0656\n",
      "Epoch 10/20\n",
      "11314/11314 [==============================] - 36s 3ms/step - loss: 0.0554\n",
      "Epoch 11/20\n",
      "11314/11314 [==============================] - 36s 3ms/step - loss: 0.0461\n",
      "Epoch 12/20\n",
      "11314/11314 [==============================] - 37s 3ms/step - loss: 0.0454: 0s - loss: 0.\n",
      "Epoch 13/20\n",
      "11314/11314 [==============================] - 36s 3ms/step - loss: 0.0377\n",
      "Epoch 14/20\n",
      "11314/11314 [==============================] - 36s 3ms/step - loss: 0.0381\n",
      "Epoch 15/20\n",
      "11314/11314 [==============================] - 40s 4ms/step - loss: 0.0359\n",
      "Epoch 16/20\n",
      "11314/11314 [==============================] - 41s 4ms/step - loss: 0.0323\n",
      "Epoch 17/20\n",
      "11314/11314 [==============================] - 37s 3ms/step - loss: 0.0350\n",
      "Epoch 18/20\n",
      "11314/11314 [==============================] - 35s 3ms/step - loss: 0.0305\n",
      "Epoch 19/20\n",
      "11314/11314 [==============================] - 35s 3ms/step - loss: 0.0283\n",
      "Epoch 20/20\n",
      "11314/11314 [==============================] - 35s 3ms/step - loss: 0.0256\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1564a863668>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Training\n",
    "model.fit(x_train_2, Y_train, batch_size=batch_size, epochs=nb_epochs,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nnDeep Neural Network - Train accuracy: 0.999000\n",
      "nDeep Neural Network - Test accuracy: 0.820000\n",
      "nDeep Neural Network - Train Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       480\n",
      "           1       1.00      1.00      1.00       584\n",
      "           2       1.00      1.00      1.00       591\n",
      "           3       1.00      1.00      1.00       590\n",
      "           4       1.00      1.00      1.00       578\n",
      "           5       1.00      1.00      1.00       593\n",
      "           6       1.00      1.00      1.00       585\n",
      "           7       1.00      1.00      1.00       594\n",
      "           8       1.00      1.00      1.00       598\n",
      "           9       1.00      1.00      1.00       597\n",
      "          10       1.00      1.00      1.00       600\n",
      "          11       1.00      1.00      1.00       595\n",
      "          12       1.00      1.00      1.00       591\n",
      "          13       1.00      1.00      1.00       594\n",
      "          14       1.00      1.00      1.00       593\n",
      "          15       1.00      1.00      1.00       599\n",
      "          16       1.00      1.00      1.00       546\n",
      "          17       1.00      1.00      1.00       564\n",
      "          18       1.00      1.00      1.00       465\n",
      "          19       1.00      1.00      1.00       377\n",
      "\n",
      "    accuracy                           1.00     11314\n",
      "   macro avg       1.00      1.00      1.00     11314\n",
      "weighted avg       1.00      1.00      1.00     11314\n",
      "\n",
      "nDeep Neural Network - Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.76      0.79       319\n",
      "           1       0.71      0.74      0.72       389\n",
      "           2       0.73      0.72      0.72       394\n",
      "           3       0.72      0.67      0.69       392\n",
      "           4       0.66      0.86      0.74       385\n",
      "           5       0.84      0.77      0.81       395\n",
      "           6       0.78      0.85      0.82       390\n",
      "           7       0.85      0.87      0.86       396\n",
      "           8       0.96      0.92      0.94       398\n",
      "           9       0.93      0.93      0.93       397\n",
      "          10       0.97      0.96      0.96       399\n",
      "          11       0.94      0.91      0.93       396\n",
      "          12       0.75      0.70      0.72       393\n",
      "          13       0.81      0.86      0.83       396\n",
      "          14       0.92      0.89      0.90       394\n",
      "          15       0.87      0.87      0.87       398\n",
      "          16       0.74      0.89      0.81       364\n",
      "          17       0.98      0.84      0.91       376\n",
      "          18       0.81      0.64      0.71       310\n",
      "          19       0.62      0.64      0.63       251\n",
      "\n",
      "    accuracy                           0.82      7532\n",
      "   macro avg       0.82      0.81      0.82      7532\n",
      "weighted avg       0.82      0.82      0.82      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Model Prediction\n",
    "\n",
    "y_train_predclass = model.predict_classes(x_train_2,batch_size=batch_size)\n",
    "y_test_predclass = model.predict_classes(x_test_2,batch_size=batch_size)\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "print (\"nnDeep Neural Network - Train accuracy: %f\" % (round(accuracy_score( y_train, y_train_predclass),3))),\n",
    "print (\"nDeep Neural Network - Test accuracy: %f\" % (round(accuracy_score( y_test,y_test_predclass),3))),\n",
    "print (\"nDeep Neural Network - Train Classification Report\")\n",
    "print (classification_report(y_train,y_train_predclass))\n",
    "print (\"nDeep Neural Network - Test Classification Report\")\n",
    "print (classification_report(y_test,y_test_predclass))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
